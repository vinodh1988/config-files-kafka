Apache Kafka 

   Open Source

   Distributed  - it is bound to be highly scable [Horizontal scalability]
   
   
   Event Streaming and  Messaging System


What cases?

    Microservices architecture? [Application Architecture]
    Distributed Systems [Infrastructure architecture]

   Monolithic architecture
   Client Server System

 In distributed systems, microservices [applications & services ] are 
 deployed at a various places in a loosely coupled manner.

 but logically they are interconnected? - Yes

 There must be C O M M U N I C A T I O N  between the services ? - Yes

 is it synchronous or asynchronous - Asychronous approach

 If this case ? we need a medium that receives the message,systematically hold the message
and delivers it to the subscriber[ service that need to get the message] whenever
it is ready

 this medium that is designed to hold the messages and handles the messages
 from the published and delivers the messages to the subscriber
 is what we refer as Messaging system/ event streaming system/message broker.

 Messaging Systems - INTEGRATION

 ---------------------------------------------------------------------------

  Order --? {order service}(publisher)
        {orderid, name, price, invoice amt}  ->message system(kafka) --- {email service} (subscriber)-> {customer}

         {order service} (publisher)
         {orderid,name,price,invoice amt} -> message System[Kafka](topic)->  (streaming-pipeline)  -> {topic}        ->(XML) {order DB Service}->DB
   
   ----------------------------------------------


   1. Kafka is a distributed event streaming and messaging sytem

   Messaging system? -

     Temporarily hold/store messages in Data Structure - (topic/queue)

     point to point - Queue
     one/many publisher - one/many subscriber - Topic

     In kafka - The Structure that holds the messages is Only Topic

     Kafka messaging system - it is a collection of topics

   Architecture?

       it is a distributed system, 
       it uses a group of nodes/servers/members to hold the topics
       there is no master /slave setup in kafka, every node is considered equal in terms of functionality

       In kafka architecture, the server/node is called a broker

       a kafka distributed system is a called as  Kafka Cluster

       -------------------------------------------------------------------------------

Terms
----------

      Cluster -   Collection of Brokers
      Broker - an individual node/member/server in the cluster
      Topic - is a structure/unit in which messages are stored

      Producer - The program/component that publishes the message to the topic
      Consumer - The program/component that subscribes/receives message from the topic

      Cluster State/ Details -  Two choices
                   1. Zookeeper -(zookeeper itself is distributed)
                   2. Kakfa itself can manage the cluster using storage


      Partition 

      ---------------------------------------------------------------------------------

      Cluster State? - there is no master/slave architecture in kafka
      but there must be a mechanism to know the number of brokers thats
      are running or part of the cluster and their state

      initially Kafka used to tackle this with help of another distributed
      cluster management/monitoring system called apache zookeeper

      if broker1 starts , it advertises itself to zookeeper and zookeeper keeps track of
      the broker1 and broker1 will try to get the cluster information from zookeeper

      similary if broker starts, it also advertises itslef to zookeeper and gets
      info about other nodes and try reachout other brokers and the redistribution
      kafka topics will take place



broker1 (server1) zookeeper node (server1), broker2 (server2) zookeeper node(server2) ,broker1(server3)

one node zookeeper

two node zookeeper

or

you can deploy zookeeper nodes saperately in server 4 and server5

----------------------------

Scalability -

 Horizontal  Scalability


       Increasing the number of brokers

----------------------------------------------------------

Partition
---------

 Cluster-> Collection of brokers

 Topic -> unit that stores messages
      [For the messages to be available always/reliable always] - messages need to be replicated



 a Topic is a storage that holds messages usually divided in one or more partitions

 Example: a topic called micromessages is using 3 partitions
   meaning: the data of the topic will be split into three parts , which each part
   might be holding some messages

      example: a topic called emailinfo is using 1 partition

      meaning: all the messages is stored in only one partition
       disadvantage: more the messages  , it becomes hard to replicate and lot
       of wastage of data,because all the messages need to be replicated


if there are 3 partitions

Brokers -b1, b2,b3
Topic -Topic1 - (p1,p2,p3)

ReplicationFactor - number of replications of the partion

if replication factor is 2


broker1-  (p1,p2)
broker2 - (p1,p3)
broker3 - (p2,p3)

in the above distribution , you could see each partition is maintaining two replications
even if one of the broker fails it ensures that all three partitions are avaliable


if broker 3 fails, redistribution of the partitions to ensure the replication factor 2  happens

broker1 - (p1,p2,p3)
broker2 - (p1,p2,p3)


Eventhough you have only broker still your topic can have three partitions

but you cannot replication factor more than one , because there is only broker


Topic has a physical storage

   that is the messages are stored in the individual brokers

   the storage is divided into partitions as per the topic specification

   topic restribution makes the topic messages distribute messages across brokers





user -> Mysql>{mysql connector} ---> [topic] --> {mongodb connector} -> Mongodb

Data pipelines
-----------------------------------------------------------------------------------


Cluster
-------

Multi node cluster

  1.Zookeeper
  2 Without Zookeeper


  Zookeeper based multi node cluster:
  ------------------------------------

  Do we have multiple servers [VMs]?

     One server itself can be used to create multiple brokers
     though not recommended, for practice requirements
     we can emulate multiple brokers in one server itself
     by using different host+port combination


     if there are multiple vms

     ip address 1 + 9092
     ip addess of vm2 + 9092
     ip address of vm3 + 9092

     ----------------------

     if there is only one system

     ip address1+ 9092
     ip appdress1+ 9093
     ip address1+9094


     In the same system we deploy zookeeper as one node 

     ----------------------

     Exercise1
     ----------

       Zookeeper based multi broker cluster

       Zookeeper : 1 node
       Kafka - 3 brokers

       Software: kafka- binary

       Zookeeper and Kafka Brokers setup
       ---------------------------------

       Zookeeper setup

       1 node zookeeper

       zookeeper server runs based on configuration provided
       in a file called zookeeper.properties

       zookeeper will maintain the state of cluster in the
       storage in tha path that specify as zookeeper path


       Broker setup
       ------------

       Each broker is started based on the configuration
       provided in a file called as server.properties

         What does the file contain?

              * It contains broker id
              * It contains host details [ip/dns of the broker]
              * it contains port on which broker runs
              * it contains default setting for topics created in the cluster
              * it contains retention details
              * It contains details of zookeeper server to register as broker
              * It contains the path for kafka-data folder to store the kafka
              topics and partition details


For our setup, for three brokers we use three files

 server1.properties
 server2.properties
 server3.properties
-------------------------------------------------------------------


Producer?

     Topic is part of the cluster

     Ther is only one Structure/Unit available in Kafka cluster
     to hold messages ie., Topic

     Kafka is Multi producer and Multi Consumer Model

    *** At any Given instance , there can be one or more producers for a topic

     Producer ? 
        * it can be a microservice
        * it can be from a program
        * it can be through command
        * it can be through api call 

        Basically Summary is producer is a client to
        kafka cluster that sends messages to topic

    *** Producer information is not taken care or maintained by kafka cluster

    *** Producers can produce messages in various formats
    and we have to use appropriate serailizer to store the messages
    and consumer when it reads it , it deserializer with appropriate
    deserialize

    *** Producers when producing message, to 
    make the message handling efficient, they can
    associate the message with a Partition Key

     Partition Key -  Can be assumed as an unique identifier
     for a group of messages
           Partition key essentially doesnot need to be a number
           it can be a string, object, json...

topic:  greetmessages

           production:"Hi How are you" ->partition 1
           testing: "How do you do" -> partition 0
           production: "Hi Hello" -> partition 1
           development: "Hey!"->partition 1
           testing: "Hello! How do you do" -> partition 0
           in the above case, production is the partion key
                       Hi, how are you is the message

        Kafka ensures that  all the messages associated with a particular
        partition key will be stored in the same partition
    -------------------------------------------------------------------

    
    Kafka-

         Distributed environment..fast and highly scalable environment

         Topic - distributed across brokers

                with the help of partitions and their replications

                and messages would be stored in partitions


                Goal: fast exchange messages/large scale of messages


       MicroService <queue>---> {topic}-->{queue}->Microservice

    

    Acknowledgements?
-------------------------

    microservice -1 -> notification -> [topic] -> [Consumer] -> [All users 100000]

    mircorservice -3 -> transaction notification -> topic -> consumer -> [appropriate User]
      [Acknowledgment]

if a message is created with a partition key (ex:  app23) for the first time

   kafka might post the message to any of the available partitions , (example partition 0)

from now on any message posted with key app23, will always get stored in partition 0

it is a not rule that only one partition key might be associated with a partition, certainly there can be many partition keys associated with a single partion

Consumer
--------

Consumer consumes messages from a topic in order

by default if we start a consumer, it will try
to read only the new produced messages , it wont consume the
older messages

but you can always read messages from the beginning or 
from particular offset or the latest


c1,c2,c3-> topic [p1,p2,p3]

Consumer if not dedicated explicitly to a partition it reads messages from all partitions

Consumers can be explicitly made to read a particular partition

Consumers if not explicitly specifying the group name still a seperate group will created
for the consumer




app24---> partition 0

parellel processing?
c1-> p1
c2-> p2
c3-> p3

c1,c2 and c3 must belong to same consumer group



Consumer FAQ?

----------------

A Topic can have as many consumers as you want

A Consumer can be associated with more than one topic

A Consumer group can also have only one consumer

every consumer if not associated with group , still a group is created,
   - in this case, if consumer goes inactive/dead the group will also go inactive


There is no CLI command dedicated to list out  consumers or producers

But for consumers you can read consumer group details

when you describe group, you can find out list of consumers in that group



----------------



Consumers - [c1,c2,c3]
             [host1,host2,host3]
            group - firstgroup

            partitions - [0,1,2]

            c1-0  c2-1 c3-2

            --offset is always with respect partition not with respect to topic


            Consumer while created , you can dedicate it to read it from a particular
            partition



--------------------------------------------------------------------------
Java
------

        * kakfa-clients
                     
                     * consumers
                     * producers

        * Kafka-streams


   input-topic  ->   {kafka-streams}(program) ---> output-topic

       kafka-stream program/service is meant to be
       live and continously program

* to process any kind of input-topic we dont need to think 
in consumer perspective
and similary to put the processed data into output-topic
we dont need to think in producer 

all we need to write is logic of the 
transformation happening in the middle

generally

   map
   filtering
   reduce
--------------------------------------------------------------

Spring-boot
-----------

Spring-kafka

   * Clients (Producers, Consumers)

   * Streams (Kafka Streams)


Kafka Producer with spring boot 

      Producer Configuration - application.properties
        if your app deals with a single producer configuration

      if you want mulitiple configuration in case
      if you want to multiple producers for various
      cases, you have to go for java configuration.



Serialization
--------------

if we are implementing producer
by using core kafka-clients without using spring-kafka

we need to give implementation for any custome
serializer, jsonSerializer


Example 1
----------

  Producer config ? - yes ( in properties file, java configuration)

  kafka template - it follows producer config to create a producer


  the producer created in the first examples
  writes message to a topic with string,jsonSerializer format

  the topic has three partitions...

  the message might be written on any of the partitions
  
  
Example 2
---------

Scenario
----------

   topic - tasks
   partitions - 3 partitions - 0,1,2

   business case - we have three projects
      
      erp, api and db

  erp tasks in partition 0 - partition key - erp
  api task in partition 1 - partition key - api
  db task in partition 2 - partition key -  db

  --------------------------------------------


    
    Consumers 
    ------------

      Same as producers , consumers can also be created
      either by using properties in application.properties
      or java configuration

      Spring-Kakfa Uses a highly customized way of
      dealing with consumers with a feature called KafkaListener

      KafkaListener is continously running program
      that receives message from Kafka Topic or topics

      If kafka-client implementation?
      --------------------------------
    KafkaConsumer object -> you read
      ConsumerRecord -> where consumer not only consumes the data,
      but also captures the metadata about the message


    -------------------------------------------------------------


    Producer?

       produced  -> person message -messages topic  with  sno->6

       consumer- CLI? - consumer consumer but it is a different group


    Consumer group - people group

    currrent - no consumer is running in the group

    history - people group consumer ran in the past and read messages
    until offeset 5 that is message -  sno->5

    Note: since there is only one consumer in the group
    it reads message from all the partitions

Now: when we instantiate a consumer in the group called people group

it will started from the existing



Consumer Example -2 
------------------------


Three consumers , that is three listeners is going to be created
and all the listeners would be belonging to the same group

Note: if you dont chose to read from a particular partion, kafka
will allocate the partitions to the available consumers

But in this scenerio we are explicitly going to specify which
listener reads which partition, we are going to only the message
but not the key



In sync Replicas
==================

All replicas [partitions] essentially doesnot need to be in sync replicas

In sync replicas  are the replicas are the replicas where the messages
are immediatly written

where as non ISRs  the messages are written eventually


Producer Acks

No Acknowledgement - [Availability of the message not assured]
Acknowledging when written into leader - moderate level of availability assured
Acknowledging when the message is written into all Isrs - highest level of availability assurance


Kafka-connect
--------------

Kafka-connect is a plugin

this plugin behaves as a mediator b/w a particular
software/db/data system  and kafka topic[cluster]

If this kafka-connect  reads data from external software
and writes it to a topic, the plugin is consider
as source

if this kafka-connect reads data from topic and
writeds it to the external software, in that case
it is considered sink


File connect 
------------

Source is text file

connect will read data from the text as and when
you write data

the written data will be transferred to a topic


Sink is also a text file

Connect will read data from topic as and
when data written into the topic

it moves the data to the destination file

KAFKA Connect - scenario
--------------------------


Problem
--------

  There is a mysql database

  it has a db called sdb,  in which it has a table called transactions

  we want to trace the transaction table and everytime
  a record is written on to the table we want
  to store that information in the form of json
  into ta topic in the kafka cluster

  it is tricky to track down and implement it by using
  producer and jdbc combination because we may frequently
  need to query the table to check the changes in the db

  But the entire problem can be easily solved using kakfa jdbc connect


  We can hava a  kafka jdbc connect plugin
  and we can have a source configuration
  and topic to store the messages and run the kafka connect


  Kafka-Streams
  --------------

  Kakfa Streams

  usually here we build a pipeline with defined
  structure by specify source and desitination topics


  Scenario1: 
  
  we create a stream that process
  1 input topic 
  and in the kafka table

  kafka table: it  is a record of
  key value pairs that is read from source
  and processed and ready to be streamed

  streamed table will be usually written
  to destination topic


  example:

  we are going to read a source topic
  as and when a message produced
  and it will create a partition key and value
  and writes it to the desition topic

  here partition key is word and values is count of number of occurance


  example;

input topic: sourcetopic

java is good language


after process the kafka table will produce message with key and value as

java 1
is 1
good 1
language 1

if second message is pushed

java is a object oriented


java 2
is 2
a 1
object 1
oriented 1


We need to maintain a streaming Config that contains
the details about the application name, broker details
plus key and value serialization/deserialization configuration

and the structure of the stream process -> topology

Usual Stream/collection operations
-----------------------------------


Map

   each item is processed and an output for each item is produced

    for n input -> there will ne outputs

    When to use: when you want to transform an income object
   
FlatMap

    each item is may further decomposed and might be producing
    a collecion/stream of values

    for n input-> there might n + x outputs

if an input record is say
   india is in asia ---> india, is, in,asia



Reduce

     for aggregation operations, we usually use a reduce

     we usually send two inputs 

     current value, result of last iteration  and produce finally
     only one output



Filter

    for n inputs
    we might having a predicate logic
    we filter out records that match the predicate logic

-----------------------------------



explaination of the topology of problem 1
------------------------------------------


input is  no key, value

value---> flatten it

example
----------

java is a language ->

  nokey java
  nokey is
  nokey a 
  nokey language

  After flattening, we group it based on value

  then we count the occurances
  and redirect stream to destination topic


Problem2

-------------

A kafka source topic -> that receives log messages


kafka stream process - receives the message
            checks for the words in the messages for suspicious activity
            moves the suspicios message to

A Kafka destination topic - that receives only suspicious message


  Note:
  ------

  When we run for the first time, right from first
  message all the existing message will be process
  and stream will be running and waiting for new
  message to proces

  in case if the stream is stopped and restarted
  with the same application id

  the stream will continue reading from the last message
  thats read 

  how? -- internally kafka stream creates consumer
  to consume the message and a consumer group with app name
  is created and consumer offset is maintained

  ------------------------------------------------------------------

  Problem
  ------------


       Users post reviews on products

       the product review by an user will posted to kafka topic

       based on the product id we would want to aggregate the
       rating and store the results in a destination topic
       
       may be from the destination topic the applications
       might be consuming the data live and produce live dashboards 

       the whole activity need to be  a streaming live activity


       in this case
source:
       the incoming message will have both key and json object

key: productid
       jsonobject : review object represented in json structure


Destination:
       the processed data will produce a result with key and value

        key: productid
        double: the aggregate value of rating of the product id



KAFKA GUI tools
-----------------

Kafka Streams
---------------

Scenario4
----------

Two Different topics

Having same kind of keys? (logically same keys)

Values? - can be of different SERDE types


topic1 - string, review
topic2 - string, sales

kafka stream processing - string, review, sales

restult topic  - string, resultmodel (review joined with sales object)

simple example as case study


KafkaStream?

       a stream will process the message if and only if
       there is an incoming message into the source topic

       in case of left outer join

       all the entries of left topic
       regardless of match with the right topic
       will be processed and stored

       all the right topics need to have
       a match withe latest left topic
       entry for join operation

       -------------------------------------------


       in case of equi joins

       say there are twon input topics

       left topic produces a message with key - key123

       it cannot be processed straight way because
       there must be an equivalent message with the same key coming
       in right topic

       we must have a time duration to look for a message
       coming into the right topic with same key [ex duration: 5 min, 10 min]

       for example within 5 min, if we get a message in
       right topic with key key123

       for example : left topic key123: "rahul"
                after 2 min: right topic key123: "Rajesh"

                key123 : "rahul Rajesh" ->destination topic
        
                

   --------------------------------------


   in some cluster environment

   the individual brokers might
   be distributed in different geo locations
   as a result the latency might increase

   broker to broker communication might demand more
   time

   and some brokers may be closely placed

   if the partition allocation are across
   the brokers that are placed in different locations
   that impacts the latency 

   we can explicilty re allocate the partitions
   to closely placed brokers by recommending the brokers
   for the partition for a given topic

   this can be achieved by acheiving a tool called
   kafka-reassign-partitions

   and similary if a leader replica is
   not easily reachable and isrs are also placed
   in such way that they are not close
   it impacts the over all performance
   
   in such cases you can explicitly  trigger
   leader election by recommending leader partitions
   in particular broker

   Note: Partition reallocation automatically
   trigger re election

   Broker election
   --------------

   Leader replicaa Partition -> on a particular broker

   Follower replicae partitions -> particular broker

   each follower replicat partititions sends a heart beat to leader replice
   frequently witihin the timeout they need to get acknowledgment from the leader

   if a heartbeat is not getting a response, it suggests there is no leader
   partition for the topic

   remember if there is no leader partition read and write operations will fail

    typical error, leader relica partition is not yet ready

    the follower will trigger election and it will announce itself as the candidate


    Kafka cluster?

    ---------------

    Kafka raft Cluster

   ====================


      